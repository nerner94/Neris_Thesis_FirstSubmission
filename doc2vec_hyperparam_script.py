# -*- coding: utf-8 -*-
"""doc2vec_hyperparam_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8mYlaiwrtPHd2ETx-6gj5pUiyiTiGTA
"""

from google.colab import drive
drive.mount("/content/drive")

import os
os.chdir("/content/drive/MyDrive/THESIS_May15")

#other libraries to import

from gensim.models.doc2vec import TaggedDocument, Doc2Vec
import multiprocessing

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

import pandas as pd
import numpy as np
from numpy.linalg import norm

class TaggedRedditDocument(object):
    def __init__(self, redditdoc):
        self.redditdoc = redditdoc
    def __iter__(self):
        for i, text in enumerate(self.redditdoc):
            yield TaggedDocument(word_tokenize(text.lower()), [i])

def long_doc2vec_stuff(dataset, embedding, i):

  tagged_document = TaggedRedditDocument(dataset)
  
  cores = multiprocessing.cpu_count()
  
  model = Doc2Vec(dm = 0, dbow_words = 0, size = embedding, iter = 10, workers = cores)
  print(str(model))

  model.build_vocab(tagged_document)
  model.train(tagged_document, total_examples = model.corpus_count, epochs = model.iter)    
  model.save("d2v_model_embdsize_{i_str}.model".format(i_str = str(embedding)))
  return model